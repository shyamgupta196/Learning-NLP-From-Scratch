Chatbots are a hot topic in industry, almost all companies integrate chatbots in their app and website. Having seen the growth of LLM's and CHAT-GPT revolutionizing the industry, we obviously cannot be swept in the wave. Let us also be a part of the wave and make contributions by learning NLP.

# NLP Resources for complete beginners who wants to make chatbots from NLP

### Learn NLP Together :)

<b> My AIM - At the end of this repository, I want to make every beginner like me, to be comfortable with NLP. </b>
<br>
Let us look at the bigger picture, Everyone should have these checkpoints -
1. I want a roadmap to learn and track my progress. write in easy to understand language.
2. Get hands dirty as soon as possible.
3. Make a Complex project.
4. Make a real world ChatBot with Python on real world data.

09 July 2023 (day-1)

1. https://towardsdatascience.com/roadmap-to-natural-language-processing-nlp-38a81dcff3a6 - This article was great to get me started with [BASICS](./basics/Basics.md)

2. good read-  https://varshasaini.in/complete-roadmap-to-learn-natural-language-processing-in-2022/#Prerequisite_to_Learn_NLP

3. https://spacy.io/usage/spacy-101 - read only till Architecture, after that nothing makes sense for beginners.

4. Activation Functions - live stream on youtube https://youtube.com/live/2VRsSWbPeB4

kaggle notebook - https://www.kaggle.com/code/shyamgupta196/activation-functions-with-pytorch 

Also checkout loss functions notebook in the above kaggle notebook.

10 july 2023- 

#### What are optimisers - ?
Think of the optimizer as a coach who guides the neural network to become better.

Example: Imagine you are blindfolded and trying to find the lowest point in a hilly area. You take small steps downhill, collect feedback from the ground, and adjust your steps based on that feedback to eventually reach the lowest point.

Remember, optimizers are like different strategies coaches use to train a neural network. They help the network improve its performance over time.

**Adam is known to converge faster than SGD in many cases, which means it reaches the optimal parameters more quickly.**

#### Why stop training before it ENDS?

because convergence leads to overfitting and hence we tend to stop training using **EARLY STOPPING**. when model does not improve for a few epochs, its a good practice to stop training, usually the epochs to wait before the stopping is called as **patience** which can be easily changed as a hyper parameter in **optimiser**.



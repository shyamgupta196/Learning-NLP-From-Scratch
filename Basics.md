These fancy terms needs to be explained properly, or else you will end up in an endless loop of tutorials.

Tokenization - Say you have a pizza, you cannot consume whole pizza at once, can you ? So, we break it into pieces. similarly, we break sentences into small chunkiesss, called <i>'tokens'</i>.


1. tfidf - Helps us figure out, most important words in a text. its like VIP list of words.

2. stemming & lemmatization - This is where you take 'tokens' to their base form. example- 'boys are running on field' -> 'boy run on field'. Stemming does not preserve the original structure of the 'token', example- 'running' -> 'runn', hence we prefer <b>'lemma' with 'tokens'</b>. Breaking lego pieces into constituents.

3. word embeddings - these are used for representing words as numbers.